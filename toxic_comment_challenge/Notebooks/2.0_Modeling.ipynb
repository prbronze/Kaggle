{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T11:49:30.885462Z",
     "start_time": "2020-11-24T11:49:29.662522Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../src/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,GridSearchCV,StratifiedShuffleSplit\n",
    "from sklearn.metrics import   roc_auc_score,multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier,RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "submission_file = pd.read_csv( '../.data/sample_submission.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from features.build_features import text_stats_features\n",
    "from visualize.plot_learning_curve import plot_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For now we want to have a glance at a baseline model, so we are not using nothing besides the comment_text with TF-IDF and a Logistic Regression with no tunning\n",
    "\n",
    "- Not using the features created in 'EDA.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T11:49:33.115061Z",
     "start_time": "2020-11-24T11:49:33.081237Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def import_train_data():\n",
    "    Xtrain = pd.read_csv('../.data/train_new_features.csv',index_col='id')\n",
    "    Xtr = Xtrain[['comment_text']]\n",
    "    ytr = Xtrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']]   \n",
    "    return Xtr,ytr\n",
    "\n",
    "def feature_extraction(Xtr,n_grams=(1,1),max_features=10000):\n",
    "    word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        use_idf=True,\n",
    "        strip_accents='unicode',\n",
    "        stop_words='english',\n",
    "        min_df=20,\n",
    "        ngram_range=n_grams,\n",
    "        max_features=max_features)\n",
    "    \n",
    "    start = time.time()\n",
    "    print('Extracting fetures...')\n",
    "    train_features = word_vectorizer.fit_transform(Xtr['comment_text'])\n",
    "    print(f'Time elapsed: {time.time() - start} seconds. ')\n",
    "    param = word_vectorizer.get_params()\n",
    "    return train_features,param\n",
    "\n",
    "def train_model(Xtr,ytr,df_results,param_extractor,c_weight='balanced'):\n",
    "    \n",
    "    clf = LogisticRegression(class_weight=c_weight) #'balanced'\n",
    "    param_classifier = clf.get_params()\n",
    "    score_df = pd.DataFrame(columns=ytr.columns)\n",
    "    scores = []\n",
    "    \n",
    "    for col in ytr.columns:\n",
    "        cv_score = np.round(np.mean(cross_val_score(clf,\n",
    "                                           Xtr,\n",
    "                                           ytr[col],\n",
    "                                           cv=3,\n",
    "                                           scoring='roc_auc')),4)\n",
    "        scores.append(cv_score)\n",
    "        print(f'CV score for label {col} is {cv_score}.')\n",
    "        \n",
    "    scores.append(param_extractor)\n",
    "    scores.append(param_classifier)\n",
    "    df_results = df_results.append(dict(zip(df_results.columns,scores)), ignore_index = True)\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T11:49:38.570572Z",
     "start_time": "2020-11-24T11:49:37.599793Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "x_train, y_train = import_train_data()\n",
    "list_cols = y_train.columns.to_list()\n",
    "list_cols.append('parameters_extraction')\n",
    "list_cols.append('parameters_classifier')\n",
    "score_df = pd.DataFrame(columns=list_cols)\n",
    "#x_train_transformed,parameters_feat_ext = feature_extraction(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test different class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T23:37:59.109193Z",
     "start_time": "2020-09-29T23:35:45.247269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for label toxic is 0.9675.\n",
      "CV score for label severe_toxic is 0.9851.\n",
      "CV score for label obscene is 0.9832.\n",
      "CV score for label threat is 0.9825.\n",
      "CV score for label insult is 0.9744.\n",
      "CV score for label identity_hate is 0.9732.\n",
      "CV score for label toxic is 0.9657.\n",
      "CV score for label severe_toxic is 0.9828.\n",
      "CV score for label obscene is 0.9813.\n",
      "CV score for label threat is 0.9819.\n",
      "CV score for label insult is 0.9729.\n",
      "CV score for label identity_hate is 0.9702.\n",
      "CV score for label toxic is 0.9624.\n",
      "CV score for label severe_toxic is 0.9806.\n",
      "CV score for label obscene is 0.9793.\n",
      "CV score for label threat is 0.9801.\n",
      "CV score for label insult is 0.9706.\n",
      "CV score for label identity_hate is 0.9674.\n"
     ]
    }
   ],
   "source": [
    "weights = [{0:1,1:1},{0:1,1:10},{0:1,1:100}]\n",
    "for w in weights:\n",
    "    score_df = train_model(x_train_transformed,y_train,\n",
    "                           score_df,parameters_feat_ext,c_weight=w)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T01:04:12.509482Z",
     "start_time": "2020-09-30T01:04:12.486736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>parameters_extraction</th>\n",
       "      <th>parameters_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9675</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9832</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9744</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': None, 'dual': False...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9675</td>\n",
       "      <td>0.9851</td>\n",
       "      <td>0.9832</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>0.9744</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': {0: 1, 1: 1}, 'dual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9657</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.9813</td>\n",
       "      <td>0.9819</td>\n",
       "      <td>0.9729</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': {0: 1, 1: 10}, 'dua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9624</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.9801</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>0.9674</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': {0: 1, 1: 100}, 'du...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0  0.9675        0.9851   0.9832  0.9825  0.9744         0.9732   \n",
       "1  0.9675        0.9851   0.9832  0.9825  0.9744         0.9732   \n",
       "2  0.9657        0.9828   0.9813  0.9819  0.9729         0.9702   \n",
       "3  0.9624        0.9806   0.9793  0.9801  0.9706         0.9674   \n",
       "\n",
       "                               parameters_extraction  \\\n",
       "0  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "1  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "2  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "3  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "\n",
       "                               parameters_classifier  \n",
       "0  {'C': 1.0, 'class_weight': None, 'dual': False...  \n",
       "1  {'C': 1.0, 'class_weight': {0: 1, 1: 1}, 'dual...  \n",
       "2  {'C': 1.0, 'class_weight': {0: 1, 1: 10}, 'dua...  \n",
       "3  {'C': 1.0, 'class_weight': {0: 1, 1: 100}, 'du...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Attributing different class_weights does not seem to be improving our scores so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search n_gram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T23:56:32.864569Z",
     "start_time": "2020-10-19T23:51:23.087259Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fetures...\n",
      "Time elapsed: 18.977115392684937 seconds. \n",
      "CV score for label toxic is 0.9652.\n",
      "CV score for label severe_toxic is 0.9815.\n",
      "CV score for label obscene is 0.9807.\n",
      "CV score for label threat is 0.9808.\n",
      "CV score for label insult is 0.9726.\n",
      "CV score for label identity_hate is 0.9693.\n",
      "Extracting fetures...\n",
      "Time elapsed: 36.379775047302246 seconds. \n",
      "CV score for label toxic is 0.9645.\n",
      "CV score for label severe_toxic is 0.9813.\n",
      "CV score for label obscene is 0.9804.\n",
      "CV score for label threat is 0.9793.\n",
      "CV score for label insult is 0.9722.\n",
      "CV score for label identity_hate is 0.9679.\n",
      "Extracting fetures...\n",
      "Time elapsed: 54.11032557487488 seconds. \n",
      "CV score for label toxic is 0.964.\n",
      "CV score for label severe_toxic is 0.9813.\n",
      "CV score for label obscene is 0.9803.\n",
      "CV score for label threat is 0.9792.\n",
      "CV score for label insult is 0.972.\n",
      "CV score for label identity_hate is 0.9673.\n",
      "Extracting fetures...\n",
      "Time elapsed: 32.74915313720703 seconds. \n",
      "CV score for label toxic is 0.7668.\n",
      "CV score for label severe_toxic is 0.8384.\n",
      "CV score for label obscene is 0.777.\n",
      "CV score for label threat is 0.7882.\n",
      "CV score for label insult is 0.7822.\n",
      "CV score for label identity_hate is 0.7627.\n"
     ]
    }
   ],
   "source": [
    "list_cols = y_train.columns.to_list()\n",
    "list_cols.append('parameters_extraction')\n",
    "list_cols.append('parameters_classifier')\n",
    "score_df = pd.DataFrame(columns=list_cols)\n",
    "\n",
    "for n_gram_range in [(1,2),(1,3),(1,4),(2,3)]:\n",
    "    x_train_transformed,parameters_feat_ext = feature_extraction(x_train,n_gram_range)\n",
    "    score_df = train_model(x_train_transformed,y_train,\n",
    "                           score_df,parameters_feat_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-19T23:56:35.496739Z",
     "start_time": "2020-10-19T23:56:35.481427Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>parameters_extraction</th>\n",
       "      <th>parameters_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9652</td>\n",
       "      <td>0.9815</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9726</td>\n",
       "      <td>0.9693</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9645</td>\n",
       "      <td>0.9813</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>0.9793</td>\n",
       "      <td>0.9722</td>\n",
       "      <td>0.9679</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9640</td>\n",
       "      <td>0.9813</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7668</td>\n",
       "      <td>0.8384</td>\n",
       "      <td>0.7770</td>\n",
       "      <td>0.7882</td>\n",
       "      <td>0.7822</td>\n",
       "      <td>0.7627</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0  0.9652        0.9815   0.9807  0.9808  0.9726         0.9693   \n",
       "1  0.9645        0.9813   0.9804  0.9793  0.9722         0.9679   \n",
       "2  0.9640        0.9813   0.9803  0.9792  0.9720         0.9673   \n",
       "3  0.7668        0.8384   0.7770  0.7882  0.7822         0.7627   \n",
       "\n",
       "                               parameters_extraction  \\\n",
       "0  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "1  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "2  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "3  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "\n",
       "                               parameters_classifier  \n",
       "0  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "1  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "2  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "3  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: We found no evidence that increasing n_gram range could improve our scores.\n",
    "We could add more to max_features instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:22:12.938775Z",
     "start_time": "2020-10-20T20:18:30.213510Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000.0\n",
      "Extracting fetures...\n",
      "Time elapsed: 6.39243483543396 seconds. \n",
      "CV score for label toxic is 0.9689.\n",
      "CV score for label severe_toxic is 0.9834.\n",
      "CV score for label obscene is 0.9833.\n",
      "CV score for label threat is 0.9814.\n",
      "CV score for label insult is 0.9755.\n",
      "CV score for label identity_hate is 0.9706.\n",
      "30000.0\n",
      "Extracting fetures...\n",
      "Time elapsed: 7.328493118286133 seconds. \n",
      "CV score for label toxic is 0.9689.\n",
      "CV score for label severe_toxic is 0.9834.\n",
      "CV score for label obscene is 0.9833.\n",
      "CV score for label threat is 0.9814.\n",
      "CV score for label insult is 0.9755.\n",
      "CV score for label identity_hate is 0.9706.\n",
      "50000.0\n",
      "Extracting fetures...\n",
      "Time elapsed: 7.240565061569214 seconds. \n",
      "CV score for label toxic is 0.9689.\n",
      "CV score for label severe_toxic is 0.9834.\n",
      "CV score for label obscene is 0.9833.\n",
      "CV score for label threat is 0.9814.\n",
      "CV score for label insult is 0.9755.\n",
      "CV score for label identity_hate is 0.9706.\n"
     ]
    }
   ],
   "source": [
    "list_cols = y_train.columns.to_list()\n",
    "list_cols.append('parameters_extraction')\n",
    "list_cols.append('parameters_classifier')\n",
    "score_df = pd.DataFrame(columns=list_cols)\n",
    "\n",
    "for max_feats in [1.5e4,3e4,5e4]:\n",
    "    print(max_feats)\n",
    "    x_train_transformed,parameters_feat_ext = feature_extraction(x_train,\n",
    "                                                                 max_features=int(max_feats))\n",
    "    score_df = train_model(x_train_transformed,y_train,\n",
    "                           score_df,parameters_feat_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T00:33:27.564371Z",
     "start_time": "2020-10-20T00:33:27.554623Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>parameters_extraction</th>\n",
       "      <th>parameters_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9689</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9814</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9689</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9814</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9689</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9814</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>0.9706</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0  0.9689        0.9834   0.9833  0.9814  0.9755         0.9706   \n",
       "1  0.9689        0.9834   0.9833  0.9814  0.9755         0.9706   \n",
       "2  0.9689        0.9834   0.9833  0.9814  0.9755         0.9706   \n",
       "\n",
       "                               parameters_extraction  \\\n",
       "0  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "1  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "2  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "\n",
       "                               parameters_classifier  \n",
       "0  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "1  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "2  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Now we seem to reach something, but the model does not seem to show any improvement with more than 15k terms.\n",
    "We could try now work with both max_features and n_gram_range.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T19:23:47.281749Z",
     "start_time": "2020-11-01T19:23:47.263603Z"
    }
   },
   "source": [
    "## Search max_features and n_gram_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T20:34:35.637866Z",
     "start_time": "2020-10-20T20:27:46.617611Z"
    },
    "code_folding": [
     5
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fetures...\n",
      "Time elapsed: 18.222862005233765 seconds. \n",
      "CV score for label toxic is 0.9669.\n",
      "CV score for label severe_toxic is 0.9828.\n",
      "CV score for label obscene is 0.9818.\n",
      "CV score for label threat is 0.9807.\n",
      "CV score for label insult is 0.9738.\n",
      "CV score for label identity_hate is 0.9707.\n",
      "Extracting fetures...\n",
      "Time elapsed: 35.48916292190552 seconds. \n",
      "CV score for label toxic is 0.9664.\n",
      "CV score for label severe_toxic is 0.9823.\n",
      "CV score for label obscene is 0.9813.\n",
      "CV score for label threat is 0.9808.\n",
      "CV score for label insult is 0.9732.\n",
      "CV score for label identity_hate is 0.9707.\n",
      "Extracting fetures...\n",
      "Time elapsed: 19.025423765182495 seconds. \n",
      "CV score for label toxic is 0.9691.\n",
      "CV score for label severe_toxic is 0.9839.\n",
      "CV score for label obscene is 0.9835.\n",
      "CV score for label threat is 0.9822.\n",
      "CV score for label insult is 0.9757.\n",
      "CV score for label identity_hate is 0.9713.\n",
      "Extracting fetures...\n",
      "Time elapsed: 36.50981831550598 seconds. \n",
      "CV score for label toxic is 0.9688.\n",
      "CV score for label severe_toxic is 0.9838.\n",
      "CV score for label obscene is 0.9833.\n",
      "CV score for label threat is 0.9818.\n",
      "CV score for label insult is 0.9755.\n",
      "CV score for label identity_hate is 0.971.\n"
     ]
    }
   ],
   "source": [
    "list_cols = y_train.columns.to_list()\n",
    "list_cols.append('parameters_extraction')\n",
    "list_cols.append('parameters_classifier')\n",
    "score_df = pd.DataFrame(columns=list_cols)\n",
    "# 3e4 and (1,2)\n",
    "for max_feats in [1.5e4,3e4]:\n",
    "    for n_gram_range in [(1,2),(1,3)]:\n",
    "    \n",
    "        x_train_transformed,parameters_feat_ext = feature_extraction(x_train,\n",
    "                                                                     n_gram_range,\n",
    "                                                                     int(max_feats))\n",
    "        score_df = train_model(x_train_transformed,y_train,\n",
    "                               score_df,parameters_feat_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-20T21:57:16.485965Z",
     "start_time": "2020-10-20T21:57:16.475054Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>parameters_extraction</th>\n",
       "      <th>parameters_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9669</td>\n",
       "      <td>0.9828</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>0.9707</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9664</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>0.9813</td>\n",
       "      <td>0.9808</td>\n",
       "      <td>0.9732</td>\n",
       "      <td>0.9707</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.9713</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.9688</td>\n",
       "      <td>0.9838</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0  0.9669        0.9828   0.9818  0.9807  0.9738         0.9707   \n",
       "1  0.9664        0.9823   0.9813  0.9808  0.9732         0.9707   \n",
       "2  0.9691        0.9839   0.9835  0.9822  0.9757         0.9713   \n",
       "3  0.9688        0.9838   0.9833  0.9818  0.9755         0.9710   \n",
       "\n",
       "                               parameters_extraction  \\\n",
       "0  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "1  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "2  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "3  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "\n",
       "                               parameters_classifier  \n",
       "0  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "1  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "2  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  \n",
       "3  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Found a peak at max_feats = 3e4, n_gram_range = (1,2), row 2. We will keep these hyperparameters for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase feature space with EDA notebook features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T11:52:03.046889Z",
     "start_time": "2020-11-24T11:51:35.196047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting fetures...\n",
      "Time elapsed: 18.996933460235596 seconds. \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x_train, y_train = import_train_data()\n",
    "list_cols = y_train.columns.to_list()\n",
    "list_cols.append('parameters_extraction')\n",
    "list_cols.append('parameters_classifier')\n",
    "score_df = pd.DataFrame(columns=list_cols)\n",
    "\n",
    "max_feats = 1.5e4\n",
    "n_gram_range = (1,2)\n",
    "\n",
    "x_train_vectorized,parameters_feat_ext = feature_extraction(x_train,\n",
    "                                                             n_gram_range,\n",
    "                                                             int(max_feats))\n",
    "\n",
    "# text statistics from EDA notebook\n",
    "text_statistics_features = text_stats_features(x_train)\n",
    "norm_text_stats_feats = MinMaxScaler().fit_transform(text_statistics_features)\n",
    "x_train_features = sp.hstack([x_train_vectorized,norm_text_stats_feats],format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T21:14:53.258221Z",
     "start_time": "2020-11-23T21:12:53.391569Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score for label toxic is 0.9697.\n",
      "CV score for label severe_toxic is 0.9855.\n",
      "CV score for label obscene is 0.9829.\n",
      "CV score for label threat is 0.9792.\n",
      "CV score for label insult is 0.975.\n",
      "CV score for label identity_hate is 0.972.\n"
     ]
    }
   ],
   "source": [
    "score_df = train_model(x_train_features,y_train,\n",
    "                               score_df,parameters_feat_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T19:37:00.784701Z",
     "start_time": "2020-11-01T19:37:00.753212Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>parameters_extraction</th>\n",
       "      <th>parameters_classifier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.9697</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>0.9829</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.972</td>\n",
       "      <td>{'analyzer': 'word', 'binary': False, 'decode_...</td>\n",
       "      <td>{'C': 1.0, 'class_weight': 'balanced', 'dual':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0  0.9697        0.9855   0.9829  0.9792   0.975          0.972   \n",
       "\n",
       "                               parameters_extraction  \\\n",
       "0  {'analyzer': 'word', 'binary': False, 'decode_...   \n",
       "\n",
       "                               parameters_classifier  \n",
       "0  {'C': 1.0, 'class_weight': 'balanced', 'dual':...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(score_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Statistics from the text concerning word_counts, marks such as question and exclamation ones, as well as symbols and smiles improved our CV score for most labels. Only _insult_ that presented a decrement in its CV score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves : Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T12:10:27.942377Z",
     "start_time": "2020-11-24T12:10:27.910452Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(x_train_vectorized[:10,:],y_train.iloc[:10,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-11-24T12:10:59.934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf1 = LogisticRegression(class_weight='balanced') #'balanced'\n",
    "clf2 = DecisionTreeClassifier()\n",
    "clf3 = MultinomialNB()\n",
    "clf = VotingClassifier(\n",
    "    estimators=[('lr', clf1),\n",
    "                ('rf', clf2),\n",
    "                ('mnb', clf3)], voting='soft')\n",
    "\n",
    "    \n",
    "plot_curve(clf, x_train_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes : voting 'soft' requires classifiers that have their output probabilities well calibrated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Although not presenting the worst score the 'threat' label presents a large standard deviation from the mean for the 5-fold CV score. As well the gap between mean train and test scores is the largest across labels.\n",
    "\n",
    "We could interpret it as the concept of 'threat' to be learned is not as easily generalizable as the rest of the labels. Another aspect we could question is the labeling process, what if the definition of 'threat' is not homogeneous across the group of people labelling the sentences? Not to be forgotten, we are dealing with an imbalanced problem and that we can verify by ourselves based on the data distributions we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-01T19:54:05.691407Z",
     "start_time": "2020-11-01T19:54:05.662223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>144277</td>\n",
       "      <td>157976</td>\n",
       "      <td>151122</td>\n",
       "      <td>159093</td>\n",
       "      <td>151694</td>\n",
       "      <td>158166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15294</td>\n",
       "      <td>1595</td>\n",
       "      <td>8449</td>\n",
       "      <td>478</td>\n",
       "      <td>7877</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0  144277        157976   151122  159093  151694         158166\n",
       "1   15294          1595     8449     478    7877           1405"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: 'Threat' label in fact presents the least ammount of examples, almost a third of the second most imbalanced label. When we split it in train and test in fact would make sense that it fitted well the train set while presenting large variance in the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Calibrating classifiers\n",
    "- Create functions to isolate operations\n",
    "\n",
    "- Faster implementation?\n",
    "- Test new algorithms?\n",
    "- Focus on errors?\n",
    "- Decrease amount of features 10k <"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should I plot the error metric or the au_roc curve ?\n",
    "- Detail text-wise translating the problem. bias variance diagnosed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T12:05:56.369737Z",
     "start_time": "2020-11-24T12:05:55.241101Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic CV SCORE:  0.9523\n",
      "severe_toxic CV SCORE:  0.9607\n",
      "obscene CV SCORE:  0.9602\n",
      "threat CV SCORE:  0.8908\n",
      "insult CV SCORE:  0.956\n",
      "identity_hate CV SCORE:  0.9242\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "from sklearn.metrics import make_scorer,roc_auc_score\n",
    "\n",
    "def roc_auc_func(y_true,y_proba):\n",
    "    print(y_proba[:10])\n",
    "    return roc_auc_score(y_true,y_proba[:,1]) # pega só a proba da classe 1\n",
    "    \n",
    "roc_scorer_NB = make_scorer(roc_auc_func,needs_proba=True)\n",
    "        \n",
    "    \n",
    "for col in y_train.columns:\n",
    "    cv_score = np.round(np.mean(cross_val_score(clf,\n",
    "                                       x_train_vectorized,\n",
    "                                       y_train[col],\n",
    "                                       cv=3,\n",
    "                                       scoring='roc_auc')),4) # 'roc_auc'\n",
    "    \n",
    "    print(f'{col} CV SCORE: ',cv_score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": "41"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
